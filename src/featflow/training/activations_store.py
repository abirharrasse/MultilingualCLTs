from __future__ import annotations

import torch
from datasets import load_dataset, load_from_disk
from torch.utils.data import DataLoader, TensorDataset
from transformer_lens.hook_points import HookedRootModule
from featflow.config import CLTTrainingRunnerConfig, AutoInterpConfig
from featflow.utils import DummyModel, activation_split_path
from sae_lens.tokenization_and_batching import concat_and_batch_sequences
from typing import Iterator, Optional, Union, Any
import os
from tqdm import tqdm
import torch.distributed as dist
from safetensors.torch import save_file, load_file
from pathlib import Path
from featflow import logger
from featflow.utils import DTYPE_MAP
import datasets

class ActivationsStore:
    """
    * Streams activations by: 
        - Generating and saving activations to disk (or)
        - Generating activations on the fly

    COMMENTS: 
        - We don't remove special tokens, would end up in other residual streams anyway. 
        - We add the BOS in front of each sequence, but remove it from activations, thus window becomes one token shorter.
    """

    # keep mypy happy
    cached_act_in: torch.Tensor
    cached_act_out: torch.Tensor
    cached_tokens: torch.Tensor
    cache_ptr: int
    leftover_activations: Optional[dict[str, Any]]

    def __init__(self, 
            model: Union[HookedRootModule, DummyModel], 
            cfg: Union[CLTTrainingRunnerConfig,AutoInterpConfig], 
            rank: int = 0, 
            world_size: int = 1,
            estimated_norm_scaling_factor_in: Optional[torch.Tensor] = None,
            estimated_norm_scaling_factor_out: Optional[torch.Tensor] = None
        ) -> None:

        self.cfg = cfg
        self.device = torch.device(cfg.device)
        self.model = model
        self.rank = rank
        self.world_size = world_size
        self.dtype = DTYPE_MAP[cfg.dtype]

        if isinstance(cfg, AutoInterpConfig): 
            self.shuffle = False
            self.return_tokens = True 
            self.mix_with_previous_buffer = False
        elif isinstance(cfg, CLTTrainingRunnerConfig): 
            self.shuffle = True
            self.return_tokens = False 
            self.mix_with_previous_buffer = True
        else: 
            raise TypeError("cfg must be either CLTTrainingRunnerConfig or AutoInterpConfig")
        
        # important for transformer lens
        model.cfg.use_hook_mlp_in = True 

        #TODO: We should decide, before or after ln2.hook_normalized ????

        if cfg.n_batches_in_buffer < 2 or cfg.n_batches_in_buffer % 2:
            raise ValueError("n_batches_in_buffer must be an even integer ≥ 2")

        self.buffer_batches = cfg.n_batches_in_buffer
        self.half_buffer_batches = cfg.n_batches_in_buffer // 2
        self.context_size = cfg.context_size

        self.n_train_batch_per_buffer = cfg.n_train_batch_per_buffer

        self.N_layers = model.cfg.n_layers
        # self.hook_names_in  = [f"blocks.{i}.ln2.hook_normalized"  for i in range(self.N_layers)]
        # self.hook_names_out = [f"blocks.{i}.hook_mlp_out" for i in range(self.N_layers)]

        self.hook_names_in  = [f"blocks.{i}.hook_resid_mid"  for i in range(self.N_layers)]
        self.hook_names_out = [f"blocks.{i}.hook_mlp_out" for i in range(self.N_layers)]
        # ── load tokenised dataset ──────────────────────────
        if self.cfg.cached_activations_path is None: 

            if "CausalNLP" in self.cfg.model_name: 
                self.raw_ds = load_dataset_auto(cfg.dataset_path, split="all").shuffle(seed=42)
                print(f"First sample sequence: {self.raw_ds[0]}", flush=True)
                # Create language list for each document
                self.doc_languages = [self.raw_ds[i]["language"] for i in range(len(self.raw_ds))]
            else: 
                self.raw_ds = load_dataset_auto(cfg.dataset_path, split="train")
        
            # Accept 'input_ids' if 'tokens' is missing
            if "tokens" not in self.raw_ds.column_names:
                if "input_ids" in self.raw_ds.column_names:
                    print("tokens column not found — using input_ids instead.")
                    self.raw_ds = self.raw_ds.rename_column("input_ids", "tokens")
                else:
                    raise ValueError(
                        f"Dataset {cfg.dataset_path} must contain a pre-tokenised tokens or input_ids column."
                    )
                
            # if "CausalNLP" in self.cfg.model_name: 
            #     self.raw_ds = self._truncate_documents_to_context_size_and_monitor_language(self.raw_ds)

            # first sample must be 1‑D int list / tensor
            first_tok = self.raw_ds[0]["tokens"]

            if isinstance(first_tok, torch.Tensor) and first_tok.ndim != 1:
                raise ValueError("Each 'tokens' entry must be a 1‑D tensor.")
            if isinstance(first_tok, (list, tuple)) and any(isinstance(x, list) for x in first_tok):
                raise ValueError("Nested sequences detected; expected a flat list of ints.")

            self._reset_token_iterator()
        else: 
            self._load_cached_activations()

        self._storage_in : Optional[torch.Tensor] = None
        self._storage_out: Optional[torch.Tensor] = None
        self._storage_tokens: Optional[torch.Tensor] = None
        self._yield_iter: Optional[
            Union[
                Iterator[tuple[torch.Tensor, torch.Tensor]],
                Iterator[tuple[torch.Tensor, torch.Tensor, torch.Tensor]],
            ]
        ] = None
        
        self.estimated_norm_scaling_factor_in = estimated_norm_scaling_factor_in
        self.estimated_norm_scaling_factor_out = estimated_norm_scaling_factor_out

        self.set_norm_scaling_factor_if_needed()
        # normalize buffer with proper activation scaling factor

        if isinstance(cfg, CLTTrainingRunnerConfig): 
            # important not to be called for autointerp to allow skipping (see run method)
            self._rebuild_buffers()

        assert self.cfg.train_batch_size_tokens % self.cfg.context_size == 0, "ctx size must divide train_batch_size_tokens"


    # def _truncate_documents_to_context_size_and_monitor_language(self, dataset):
    #     """
    #     Truncate each document to multiples of context_size to avoid 
    #     cross-document concatenation that mixes languages, and tracks the language of each sequence. 
    #     """
    #     new_doc_languages = []
        
    #     def truncate_document_with_idx(example, idx):
    #         tokens = example["tokens"]
    #         if not isinstance(tokens, list):
    #             tokens = tokens.tolist()
            
    #         doc_len = len(tokens)
    #         truncated_len = (doc_len // (self.context_size - 1)) * (self.context_size - 1)
            
    #         if truncated_len > 0:
    #             example["tokens"] = tokens[:truncated_len]
    #             # Calculate how many sequences this document will create
    #             n_sequences = truncated_len // (self.context_size - 1)
    #             # Add language for each sequence this document creates
    #             for _ in range(n_sequences):
    #                 new_doc_languages.append(self.doc_languages[idx])
    #         else:
    #             example["tokens"] = tokens[:1] if tokens else [0]
    #             new_doc_languages.append(self.doc_languages[idx])
            
    #         return example
        
    #     # print(f"Truncating documents to multiples of context_size={self.context_size}")
    #     truncated_ds =  dataset.map(truncate_document_with_idx, with_indices=True, load_from_cache_file=False)
    #     # print(f"Dataset size after truncation: {len(truncated_ds)}")
        
    #     # Update doc_languages to match the new sequence-level structure
    #     self.doc_languages = new_doc_languages
    #     print(f"Language mapping updated: {len(self.doc_languages)} sequences")
        
    #     return truncated_ds

    # ───────────────────  token pipeline  ───────────────────

    def _iterate_raw_dataset_tokens(self) -> Iterator[torch.Tensor]:
        """
        Yield each row's token vector as a 1‑D torch.Tensor on **CPU**.
        """
        dataset_len = len(self.raw_ds)
        shard_size = dataset_len // self.world_size
        start = self.rank * shard_size
        end = dataset_len if self.rank == self.world_size - 1 else start + shard_size
        self.runtime_doc_languages = []

        for i in range(start, end):
            toks = self.raw_ds[i]["tokens"]

            if not isinstance(toks, torch.Tensor):
                toks = torch.tensor(toks, dtype=torch.long)
            
            doc_len = len(toks)
            truncated_len = (doc_len // (self.context_size)) * (self.context_size) # TODO before -1 to both

            if truncated_len > 0:
                toks = toks[:truncated_len]
                if "CausalNLP" in self.cfg.model_name: 
                    n_sequences = truncated_len // (self.context_size)
                    for _ in range(n_sequences):
                        self.runtime_doc_languages.append(self.doc_languages[i])

            yield toks

    def _reset_token_iterator(self) -> None:
        tokenizer = getattr(self.model, "tokenizer", None)
        bos_id = None if tokenizer is None else tokenizer.bos_token_id

        base_iter = concat_and_batch_sequences(
            tokens_iterator=self._iterate_raw_dataset_tokens(),
            context_size=self.context_size,
            begin_batch_token_id=None, # we dont want to prepend bos here, we add in run_with_cache TODO
            begin_sequence_token_id=None,
            sequence_separator_token_id=bos_id,
        )

        self._token_iter = iter(self._batchify(base_iter))

    def _batchify(self, iterator: Iterator[torch.Tensor]) -> Iterator[torch.Tensor]:
        batch = []
        for item in iterator:
            if item.shape[0] != self.context_size:
                continue  # skip last sequence that might be too short

            batch.append(item)
            if len(batch) == self.cfg.store_batch_size_prompts:
                yield torch.stack(batch)
                batch = []

    def _next_token_batch(self) -> torch.Tensor:
        try:
            batch = next(self._token_iter)
        except StopIteration:
            self._reset_token_iterator()
            batch = next(self._token_iter)

        print(f"DEBUG: _next_token_batch returning shape: {batch.shape}")
        return batch.to(device=self.device, dtype=torch.long)

    # ───────────────────  activation  ───────────────────

    @torch.no_grad()
    def _activations(self, batch_tokens: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:
        
        # Manually add BOS token to ensure we have context_size + 1 tokens
        bos_token = self.model.tokenizer.bos_token_id
        bos_column = torch.full((batch_tokens.shape[0], 1), bos_token, dtype=batch_tokens.dtype, device=batch_tokens.device)
        batch_tokens_with_bos = torch.cat([bos_column, batch_tokens], dim=1)
        
        cache = self.model.run_with_cache(
            batch_tokens_with_bos,
            names_filter=self.hook_names_in+self.hook_names_out,
            prepend_bos=False, # We already added BOS manually
        )[1]
        
        def stack(names: list[str]) -> torch.Tensor:
            missing = [n for n in names if n not in cache]
            if missing:
                raise KeyError(f"The following hooks were not found in the cache: {missing}")
            acts_list: list[torch.Tensor] = [cache[n].flatten(2) for n in names]
            acts = torch.stack(acts_list, dim=0).permute(1, 2, 0, 3)
            acts = acts[:, 1:, :, :]  # Remove BOS token activations at pos 0

            assert acts.shape[1] == self.context_size
            # Flatten B and C → (B * C, N_layers, d)
            B, C, N_layers, d = acts.shape
            return acts.reshape(B * C, N_layers, d).cpu()

        return stack(self.hook_names_in), stack(self.hook_names_out)

    def _skip_batches(self):
        """Skip batches by advancing the token iterator without computing activations"""
        n_batches = self.cfg.train_batch_size_tokens // (self.context_size * self.cfg.store_batch_size_prompts)
        assert self.cfg.train_batch_size_tokens % (self.context_size * self.cfg.store_batch_size_prompts) == 0
        tokens = []
        for _ in range(n_batches):
            token_batch = self._next_token_batch()
            tokens.append(token_batch.cpu())
        
        # Return flattened tokens similar to _fresh_activation_batches
        return torch.cat([t.reshape(-1) for t in tokens], dim=0)

    def _fresh_activation_batches(self, return_tokens: bool = False, mix_with_previous_buffer: bool = True):
        n_batches = self.buffer_batches if not mix_with_previous_buffer else self.half_buffer_batches
        ins, outs, toks = [], [], []

        for i in range(n_batches):
            token_batch = self._next_token_batch()
            act_in, act_out = self._activations(token_batch)
            ins.append(act_in)
            outs.append(act_out)
            if return_tokens:
                assert token_batch.shape[1] == self.context_size, "wrong token batch size"
                # assert torch.all(token_batch[:, 0] == self.model.tokenizer.bos_token_id), "Not all sequences start with BOS token"
                toks.append(token_batch.cpu()) # Don't remove BOS token activations at pos 0, otherwise [:, 1:]

        if return_tokens:
            toks = torch.cat([t.reshape(-1) for t in toks], dim=0)  # flatten to [B * ctx]
            return torch.cat(ins, 0), torch.cat(outs, 0), toks
        else:
            return torch.cat(ins, 0), torch.cat(outs, 0)

    def _rebuild_buffers(self) -> None:

        return_tokens = self.return_tokens
        use_cached = self.cfg.cached_activations_path is not None

        if use_cached:
            result = self._load_buffer_from_cached(return_tokens=return_tokens)
        else:
            result = self._fresh_activation_batches(
                return_tokens=return_tokens,
                mix_with_previous_buffer=self.mix_with_previous_buffer
            )
        
        if return_tokens:
            new_in, new_out, new_tokens = result
        else:
            new_in, new_out = result

        if self.mix_with_previous_buffer and self._storage_in is not None:
            all_in = torch.cat([self._storage_in, new_in], dim=0)
            all_out = torch.cat([self._storage_out, new_out], dim=0)
            if self.return_tokens:
                all_tokens = torch.cat([self._storage_tokens, new_tokens], dim=0)
        else:
            all_in, all_out = new_in, new_out
            if self.return_tokens:
                all_tokens = new_tokens
            
        if self.shuffle:
            perm = torch.randperm(all_in.size(0), device="cpu") #what should it be ??? 
            all_in, all_out = all_in[perm], all_out[perm]
            if self.return_tokens:
                all_tokens = all_tokens[perm]

        if self.mix_with_previous_buffer:
            split = all_in.size(0) // 2
            self._storage_in, self._storage_out = all_in[:split], all_out[:split]
            if self.return_tokens:
                self._storage_tokens = all_tokens[:split]
        else: 
            split = 0
        
        # Apply normalization
        in_normed = all_in[split:] #self.apply_norm_scaling_factor_in(all_in[split:])
        out_normed = all_out[split:] #self.apply_norm_scaling_factor_out(all_out[split:])

        if self.return_tokens:
            dataset = TensorDataset(in_normed, out_normed, all_tokens[split:])
        else:
            dataset = TensorDataset(in_normed, out_normed)
        loader = DataLoader(
            dataset,
            batch_size=self.cfg.train_batch_size_tokens,
            shuffle=self.shuffle,
            drop_last=False
        )
        self._yield_iter = iter(loader)

    # ───────────────────  normalization  ───────────────────

    @torch.no_grad()
    def estimate_norm_scaling_factor(self, n_batches_for_norm_estimate: int = 25):
        
        norms_per_layer_in = []
        norms_per_layer_out = []

        self.estimated_norm_scaling_factor_in = torch.ones(self.N_layers, device="cpu")
        self.estimated_norm_scaling_factor_out = torch.ones(self.N_layers, device="cpu")

        for _ in tqdm(range(n_batches_for_norm_estimate), desc="Estimating norm scaling factor"):
            if self.return_tokens:
                _, acts_in, acts_out = next(iter(self))
            else: 
                acts_in, acts_out = next(iter(self))

            assert acts_in.shape[1] == self.N_layers, \
                f"Expected second dimension to be n_layers ({self.N_layers}), but got {acts_in.shape[1]}"
            
            norms_per_layer_in.append(acts_in.norm(dim=-1).mean(dim=0))
            norms_per_layer_out.append(acts_out.norm(dim=-1).mean(dim=0))

        mean_norm_per_layer_in = torch.stack(norms_per_layer_in, dim=0).mean(dim=0)
        mean_norm_per_layer_out = torch.stack(norms_per_layer_out, dim=0).mean(dim=0)

        print(f"Estimated norm scaling factor in:{(self.cfg.d_in ** 0.5) / mean_norm_per_layer_in}", flush=True)
        print(f"Estimated norm scaling factor out:{(self.cfg.d_in ** 0.5) / mean_norm_per_layer_out}", flush=True)

        return (self.cfg.d_in ** 0.5) / mean_norm_per_layer_in, (self.cfg.d_in ** 0.5) / mean_norm_per_layer_out
    
    @torch.no_grad()
    def set_norm_scaling_factor_if_needed(self):

        if self.estimated_norm_scaling_factor_in is None or self.estimated_norm_scaling_factor_out is None:
            if self.rank == 0:
                self.estimated_norm_scaling_factor_in, self.estimated_norm_scaling_factor_out = self.estimate_norm_scaling_factor()
            else:
                dummy_shape = (self.N_layers,)
                self.estimated_norm_scaling_factor_in = torch.empty(
                    dummy_shape, dtype=self.dtype, device=self.device
                )
                self.estimated_norm_scaling_factor_out = torch.empty(
                    dummy_shape, dtype=self.dtype, device=self.device
                )

            if self.cfg.ddp or self.cfg.fsdp: 
                # is there a better way ???
                self.estimated_norm_scaling_factor_in = self.estimated_norm_scaling_factor_in.to(self.device)
                self.estimated_norm_scaling_factor_out = self.estimated_norm_scaling_factor_out.to(self.device)

                if self.cfg.ddp or self.cfg.fsdp:
                    dist.broadcast(self.estimated_norm_scaling_factor_in, src=0)
                    dist.broadcast(self.estimated_norm_scaling_factor_out, src=0)

                # Move back to CPU once shared across instances
                self.estimated_norm_scaling_factor_in = self.estimated_norm_scaling_factor_in.cpu()
                self.estimated_norm_scaling_factor_out = self.estimated_norm_scaling_factor_out.cpu()

    def apply_norm_scaling_factor_in(self, activations: torch.Tensor) -> torch.Tensor:
        if self.estimated_norm_scaling_factor_in is None:
            raise ValueError(
                "estimated_norm_scaling_factor_in is not set, call set_norm_scaling_factor_if_needed() first"
            )
        scaling = self.estimated_norm_scaling_factor_in.view(1, -1, 1)
        return activations * scaling

    def apply_norm_scaling_factor_out(self, activations: torch.Tensor) -> torch.Tensor:
        if self.estimated_norm_scaling_factor_out is None:
            raise ValueError(
                "estimated_norm_scaling_factor_out is not set, call set_norm_scaling_factor_if_needed() first"
            )
        scaling = self.estimated_norm_scaling_factor_out.view(1, -1, 1)
        return activations * scaling

    def remove_norm_scaling_factor_in(self, activations: torch.Tensor) -> torch.Tensor:
        if self.estimated_norm_scaling_factor_in is None:
            raise ValueError(
                "estimated_norm_scaling_factor_in is not set, call set_norm_scaling_factor_if_needed() first"
            )
        scaling = self.estimated_norm_scaling_factor_in.view(1, -1, 1)
        return activations / scaling

    def remove_norm_scaling_factor_out(self, activations: torch.Tensor) -> torch.Tensor:
        if self.estimated_norm_scaling_factor_out is None:
            raise ValueError(
                "estimated_norm_scaling_factor_out is not set, call set_norm_scaling_factor_if_needed() first"
            )
        scaling = self.estimated_norm_scaling_factor_out.view(1, -1, 1)
        return activations / scaling

    # ------------------ Generate activations, save to and load from disk ------------------

    def generate_and_save_activations(self, path: str, split_count: int = 10, number_of_tokens: Optional[int] = None, split_begin_idx: int = 0, split_end_idx: Optional[int] = None):
        """
        path - directory where splits will be saved with name activations_ctx_{self.context_size}_split_{split_idx}.safetensors
        """
        # Set default value for split_end_idx
        if split_end_idx is None:
            split_end_idx = split_count

        save_path = Path(path)
        os.makedirs(save_path / f"ctx_{self.context_size}", exist_ok=True)

        buffer_size = self.cfg.store_batch_size_prompts * self.cfg.context_size * self.buffer_batches

        # Infer full token count if not provided
        if number_of_tokens is None:
            number_of_tokens = sum(len(example["tokens"]) for example in self.raw_ds)
            number_of_tokens -= number_of_tokens % buffer_size  # truncate to full buffers
            print(f"[ActivationsStore] Using full dataset: {number_of_tokens} tokens", flush=True)

        total_buffers = number_of_tokens // buffer_size
        usable_buffers = (total_buffers // split_count) * split_count  # drop leftovers
        buffers_per_split = usable_buffers // split_count
        split_token_count = buffers_per_split * buffer_size

        print(f"[ActivationsStore] Saving {usable_buffers * buffer_size} tokens "
            f"in {split_count} splits of {buffers_per_split} buffers each", flush=True)

        # Skip to the correct starting position in the dataset
        start_buffer_idx = split_begin_idx * buffers_per_split
        print(f"[ActivationsStore] Skipping first {start_buffer_idx} buffers to reach split {split_begin_idx}", flush=True)
        for skip_idx in range(start_buffer_idx):
            for _ in range(self.buffer_batches):
                self._next_token_batch()
            if skip_idx % 500 == 0:
                print(f"[ActivationsStore] Skipped buffer {skip_idx + 1}/{start_buffer_idx}", flush=True)

        buffer_idx = start_buffer_idx
        for split_idx in range(split_begin_idx, split_end_idx):
            acts_in = torch.empty((split_token_count, self.N_layers, self.cfg.d_in), dtype=self.dtype)
            acts_out = torch.empty((split_token_count, self.N_layers, self.cfg.d_in), dtype=self.dtype)
            tokens = torch.empty((split_token_count,), dtype=torch.long)

            for i in range(buffers_per_split):
                act_in_buffer, act_out_buffer, token_buffer = self._fresh_activation_batches(
                    return_tokens=True,
                    mix_with_previous_buffer=False
                )
                idx = i * buffer_size
                acts_in[idx:idx + buffer_size] = act_in_buffer
                acts_out[idx:idx + buffer_size] = act_out_buffer
                tokens[idx:idx + buffer_size] = token_buffer

                if buffer_idx % 50 == 0 or buffer_idx == usable_buffers - 1:
                    print(f"[ActivationsStore] Processed buffer {buffer_idx + 1}/{usable_buffers}", flush=True)

                buffer_idx += 1

            split_path = activation_split_path(save_path, self.context_size, split_idx, must_exist=False)
            save_file({"act_in": acts_in, "act_out": acts_out, "tokens": tokens}, split_path)
            print(f"[ActivationsStore] Saved split {split_idx + 1}/{split_count} to {split_path}", flush=True)

        print(f"[ActivationsStore] Finished saving all {split_count} splits to {save_path}")

    def _load_buffer_from_cached(self, return_tokens:bool = False) -> tuple[torch.Tensor, ...]:
        
        total_activations = self.cached_act_in.shape[0]

        if self.n_train_batch_per_buffer is None or self.cfg.train_batch_size_tokens is None:
            raise ValueError("n_train_batch_per_buffer and train_batch_size_tokens must not be None here")
        
        if self.cache_ptr + self.n_train_batch_per_buffer * self.cfg.train_batch_size_tokens > total_activations:
            self.leftover_activations = {
                "act_in": self.cached_act_in[self.cache_ptr:],
                "act_out": self.cached_act_out[self.cache_ptr:],
                "tokens": self.cached_tokens[self.cache_ptr:]
            }
            self._load_cached_activations()

            self.cached_act_in = torch.cat([self.leftover_activations["act_in"], self.cached_act_in], dim=0)
            self.cached_act_out = torch.cat([self.leftover_activations["act_out"], self.cached_act_out], dim=0)
            self.cached_tokens = torch.cat([self.leftover_activations["tokens"], self.cached_tokens], dim=0)

            if self.cached_act_in.shape[0] < self.n_train_batch_per_buffer * self.cfg.train_batch_size_tokens: 
                raise ValueError(
                    "Buffer size greater than split size"
                )

            self.leftover_activations = None

        start = self.cache_ptr
        end = start + self.n_train_batch_per_buffer * self.cfg.train_batch_size_tokens

        self.cache_ptr = end

        if return_tokens:
            return self.cached_act_in[start:end], self.cached_act_out[start:end], self.cached_tokens[start:end]
        else: 
            return self.cached_act_in[start:end], self.cached_act_out[start:end]

    def _load_cached_activations(self) -> None:
        if not hasattr(self, "split"):
            self.split = self.rank 

        if self.cfg.cached_activations_path is None:
            raise ValueError("cached_activations_path must not be None here")

        # Compose path
        activations_path = activation_split_path(self.cfg.cached_activations_path, self.context_size, self.split)
        
        # If it doesn't exist, restart from 0
        if not os.path.exists(activations_path):
            logger.info(f"[ActivationsStore] No split at {activations_path}, restarting from split {self.rank}")
            self.split = self.rank
            activations_path = activation_split_path(self.cfg.cached_activations_path, self.context_size, self.split)
            if not os.path.exists(activations_path):
                raise FileNotFoundError(f"No cached activations found at {activations_path}")

        # Load
        tensors = load_file(activations_path)
        self.cached_act_in = tensors["act_in"].cpu() # was saved on gpu
        self.cached_act_out = tensors["act_out"].cpu() # was saved on gpu
        self.cached_tokens = tensors["tokens"].cpu() # was saved on gpu

        logger.info(f"[ActivationsStore] Loaded split {self.split} "
                    f"with {self.cached_act_in.shape[0]} samples "
                    f"from {activations_path}")

        self.split += self.world_size
        self.cache_ptr = 0

    # ───────────────────  public iterator  ───────────────────

    def __iter__(self):
        while True:
            if self._yield_iter is None:
                self._rebuild_buffers()

            try:
                batch = next(self._yield_iter)
                if self.return_tokens:
                    token_batch, act_in, act_out = batch[2], batch[0], batch[1]
                    yield token_batch, act_in, act_out
                else:
                    act_in, act_out = batch[0], batch[1]
                    yield act_in, act_out
            except StopIteration:
                self._rebuild_buffers()

# helped to load dataset either locally or from huggingface
def load_dataset_auto(path_or_name: str, split="train"):
    if os.path.exists(path_or_name):
        return load_from_disk(path_or_name)
    else:
        if split == "all":
            # Load all splits and concatenate
            ds_dict = load_dataset(path_or_name)
            
            # Add language column based on split name before concatenation
            datasets_with_lang = []
            for split_name, ds in ds_dict.items():
                ds = ds.add_column("language", [split_name] * len(ds))
                datasets_with_lang.append(ds)
            
            # Concatenate all splits into one Dataset
            return datasets.concatenate_datasets(datasets_with_lang)
        else:
            return load_dataset(path_or_name, split=split)
            
def _filter_buffer_acts(
    buffer: tuple[torch.Tensor, torch.Tensor | None],
    exclude_tokens: torch.Tensor | None,
) -> torch.Tensor:
    """
    Filter out activations for tokens that are in exclude_tokens.
    """

    activations, tokens = buffer
    if tokens is None or exclude_tokens is None:
        return activations

    mask = torch.isin(tokens, exclude_tokens)
    return activations[~mask]
